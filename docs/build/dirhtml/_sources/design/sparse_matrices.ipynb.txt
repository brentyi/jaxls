{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Sparse matrices\n\njaxls solves nonlinear least squares using Levenberg-Marquardt, which is based on\nthe Gauss-Newton method.\n\nThis page discusses:\n- The Gauss-Newton approximation and why Jacobian structure matters\n- jaxls's `BlockRowSparseMatrix` representation\n- Linear solvers and preconditioning strategies"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from loguru import logger\n",
    "\n",
    "logger.remove()\n",
    "logger.add(sys.stdout, format=\"<level>{level: <8}</level> | {message}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "## The Gauss-Newton approximation\n\nNewton's method for minimizing $f(x) = \\frac{1}{2}\\sum_i \\|r_i(x)\\|^2$ requires the\nHessian $\\nabla^2 f$, which is expensive to compute. Gauss-Newton instead approximates\nthe Hessian using only first-order information:\n\n$$\\nabla^2 f \\approx J^\\top J$$\n\nwhere $J = \\partial r / \\partial x$ is the Jacobian of the stacked residual vector\n$r(x) = [r_0(x)^\\top, r_1(x)^\\top, \\ldots]^\\top$ with respect to variable tangent\nvectors (for Euclidean variables, this is just $\\partial r / \\partial x$).\n\n## Why Jacobians are sparse\n\nThese Jacobians are typically sparse: each residual $r_i$ depends on only a few\nvariables, so row block $i$ of $J$ has non-zeros only in columns corresponding to\nthose variables.\n\nThis sparsity pattern corresponds to a bipartite factor graph:\n\n- **Variables** (circles): The parameters we're optimizing\n- **Costs** (squares): Residual functions that connect to their dependent variables"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": "# Internal implementation.\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom IPython.display import HTML\nimport numpy as np\n\n# Create a simple factor graph visualization.\n# Variables: x0 (dim 2), x1 (dim 2), x2 (dim 2)\n# Cost types: A (unary), B (binary adjacent), C (binary non-adjacent)\n\nfig = make_subplots(\n    rows=1, cols=2,\n    subplot_titles=(\"Factor Graph\", \"Jacobian Sparsity Pattern\"),\n    column_widths=[0.45, 0.55],\n    horizontal_spacing=0.12,\n)\n\n# Factor graph layout.\nvar_x = [0, 2, 4]\nvar_y = [1, 1, 1]\nvar_labels = [\"x₀\", \"x₁\", \"x₂\"]\nvar_dims = [2, 2, 2]  # Tangent dimensions (all 2D).\n\n# C₀ placed above the variables for cleaner layout.\ncost_x = [0, 1, 3, 4, 2]\ncost_y = [0, 0, 0, 0, 2]\ncost_labels = [\"A₀\", \"B₀\", \"B₁\", \"A₁\", \"C₀\"]\ncost_dims = [2, 2, 2, 2, 2]  # Residual dimensions (all 2).\n# Colors: A costs (blue), B costs (orange), C costs (green).\ncost_colors = [\"#1976d2\", \"#f57c00\", \"#f57c00\", \"#1976d2\", \"#388e3c\"]\n\n# Connections: (cost_idx, var_idx).\n# A₀→x₀, B₀→x₀,x₁, B₁→x₁,x₂, A₁→x₂, C₀→x₀,x₂\nedges = [(0, 0), (1, 0), (1, 1), (2, 1), (2, 2), (3, 2), (4, 0), (4, 2)]\n\n# Draw edges.\nfor ci, vi in edges:\n    fig.add_trace(\n        go.Scatter(\n            x=[cost_x[ci], var_x[vi]],\n            y=[cost_y[ci], var_y[vi]],\n            mode=\"lines\",\n            line=dict(color=\"#888\", width=2),\n            showlegend=False,\n            hoverinfo=\"skip\",\n        ),\n        row=1, col=1,\n    )\n\n# Draw variables (circles).\nfig.add_trace(\n    go.Scatter(\n        x=var_x,\n        y=var_y,\n        mode=\"markers+text\",\n        marker=dict(size=40, color=\"#2196F3\", line=dict(width=2, color=\"white\")),\n        text=var_labels,\n        textposition=\"middle center\",\n        textfont=dict(color=\"white\", size=14),\n        name=\"Variables\",\n        hovertemplate=\"%{text}<br>dim=%{customdata}<extra></extra>\",\n        customdata=var_dims,\n    ),\n    row=1, col=1,\n)\n\n# Draw costs (squares) - color by cost type.\nfor i, (cx, cy, label, color, dim) in enumerate(zip(cost_x, cost_y, cost_labels, cost_colors, cost_dims)):\n    fig.add_trace(\n        go.Scatter(\n            x=[cx],\n            y=[cy],\n            mode=\"markers+text\",\n            marker=dict(size=35, color=color, symbol=\"square\",\n                        line=dict(width=2, color=\"white\")),\n            text=[label],\n            textposition=\"middle center\",\n            textfont=dict(color=\"white\", size=12),\n            name=label,\n            hovertemplate=f\"{label}<br>residual dim={dim}<extra></extra>\",\n            showlegend=False,\n        ),\n        row=1, col=1,\n    )\n\n# Build Jacobian sparsity pattern.\n# Order costs by type for the matrix: A₀, A₁, B₀, B₁, C₀\ncost_order = [0, 3, 1, 2, 4]  # Reorder to group by type.\nordered_cost_dims = [cost_dims[i] for i in cost_order]\nordered_cost_labels = [cost_labels[i] for i in cost_order]\nordered_cost_colors_idx = [0, 0, 1, 1, 2]  # A, A, B, B, C\n\nn_rows = sum(ordered_cost_dims)  # 10\nn_cols = sum(var_dims)   # 6\n\nrow_starts = [0] + list(np.cumsum(ordered_cost_dims)[:-1])\ncol_starts = [0] + list(np.cumsum(var_dims)[:-1])\n\n# Build edge map for reordered costs.\n# Original edges: A₀→x₀, B₀→x₀,x₁, B₁→x₁,x₂, A₁→x₂, C₀→x₀,x₂\n# Reordered: A₀→x₀, A₁→x₂, B₀→x₀,x₁, B₁→x₁,x₂, C₀→x₀,x₂\nreordered_edges = [\n    (0, [0]),      # A₀ → x₀\n    (1, [2]),      # A₁ → x₂\n    (2, [0, 1]),   # B₀ → x₀, x₁\n    (3, [1, 2]),   # B₁ → x₁, x₂\n    (4, [0, 2]),   # C₀ → x₀, x₂ (non-adjacent!)\n]\n\n# Create color matrix for heatmap.\ncolor_matrix = np.zeros((n_rows, n_cols))\ncolor_vals = [1, 1, 2, 2, 3]  # A=1 (blue), B=2 (orange), C=3 (green)\nfor row_idx, (_, var_indices) in enumerate(reordered_edges):\n    r0, r1 = row_starts[row_idx], row_starts[row_idx] + ordered_cost_dims[row_idx]\n    for vi in var_indices:\n        c0, c1 = col_starts[vi], col_starts[vi] + var_dims[vi]\n        color_matrix[r0:r1, c0:c1] = color_vals[row_idx]\n\n# Custom colorscale: white, blue, orange, green.\nfig.add_trace(\n    go.Heatmap(\n        z=color_matrix[::-1],\n        colorscale=[[0, \"white\"], [0.33, \"#1976d2\"], [0.67, \"#f57c00\"], [1, \"#388e3c\"]],\n        showscale=False,\n        zmin=0, zmax=3,\n        hovertemplate=\"row %{y}, col %{x}<extra></extra>\",\n    ),\n    row=1, col=2,\n)\n\n# Add grid lines for Jacobian.\nfor i in range(n_rows + 1):\n    fig.add_shape(\n        type=\"line\", x0=-0.5, x1=n_cols-0.5, y0=i-0.5, y1=i-0.5,\n        line=dict(color=\"#ddd\", width=1), row=1, col=2,\n    )\nfor j in range(n_cols + 1):\n    fig.add_shape(\n        type=\"line\", x0=j-0.5, x1=j-0.5, y0=-0.5, y1=n_rows-0.5,\n        line=dict(color=\"#ddd\", width=1), row=1, col=2,\n    )\n\n# Add block separators (between cost types).\ntype_boundaries = [4, 8]  # After A costs, after B costs\nfor rs in type_boundaries:\n    fig.add_shape(\n        type=\"line\", x0=-0.5, x1=n_cols-0.5, y0=n_rows-rs-0.5, y1=n_rows-rs-0.5,\n        line=dict(color=\"#999\", width=2), row=1, col=2,\n    )\nfor cs in col_starts[1:]:\n    fig.add_shape(\n        type=\"line\", x0=cs-0.5, x1=cs-0.5, y0=-0.5, y1=n_rows-0.5,\n        line=dict(color=\"#2196F3\", width=2), row=1, col=2,\n    )\n\n# Add column labels (variables) at bottom.\ncol_centers = [col_starts[i] + var_dims[i] / 2 - 0.5 for i in range(len(var_dims))]\nfor i, (cx, label) in enumerate(zip(col_centers, var_labels)):\n    fig.add_annotation(\n        x=cx, y=-1.0, text=label, showarrow=False,\n        font=dict(size=14), xref=\"x2\", yref=\"y2\",\n    )\n\n# Add row labels (costs) at left - show type groups.\ntype_labels = [\"A\", \"B\", \"C\"]\ntype_row_ranges = [(0, 4), (4, 8), (8, 10)]\nfor label, (r0, r1) in zip(type_labels, type_row_ranges):\n    ry = n_rows - (r0 + r1) / 2 - 0.5\n    fig.add_annotation(\n        x=-0.8, y=ry, text=label, showarrow=False,\n        font=dict(size=12), xref=\"x2\", yref=\"y2\",\n    )\n\nfig.update_xaxes(showgrid=False, zeroline=False, showticklabels=False, row=1, col=1)\nfig.update_yaxes(showgrid=False, zeroline=False, showticklabels=False, row=1, col=1)\nfig.update_xaxes(showgrid=False, zeroline=False, showticklabels=False, row=1, col=2)\nfig.update_yaxes(showgrid=False, zeroline=False, showticklabels=False, row=1, col=2)\n\nfig.update_layout(\n    height=350,\n    margin=dict(t=40, b=60, l=60, r=40),\n    showlegend=False,\n)\n\nHTML(fig.to_html(full_html=False, include_plotlyjs=\"cdn\"))"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": "In this example:\n- Type A costs (A₀, A₁) each depend on one variable → single non-zero block per row\n- Type B costs (B₀, B₁) connect adjacent variables → two adjacent non-zero blocks per row\n- Type C cost (C₀) connects x₀ and x₂ → two non-adjacent blocks with a gap between them\n- The Jacobian is ~60% zeros, and this ratio grows with problem size\n\nFor large SLAM or bundle adjustment problems, Jacobians can be >99% sparse. Exploiting\nthis sparsity is critical for scaling to large problems."
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": "## Sparse matrix formats\n\njaxls implements three sparse matrix representations. You can select between them\nusing the `sparse_mode` parameter:\n\n```python\n# Default: block-row format (best for CG on GPU).\nsolution = problem.solve(sparse_mode=\"blockrow\")\n\n# COO format (converts to JAX BCOO).\nsolution = problem.solve(sparse_mode=\"coo\")\n\n# CSR format (always used for CHOLMOD).\nsolution = problem.solve(sparse_mode=\"csr\")\n```\n\n### BlockRowSparseMatrix\n\njaxls's default and recommended sparse matrix representation uses what we call\nsparse \"block-rows\". Costs of the same type have Jacobians with identical structure:\nsame block sizes, just different values and column positions. jaxls exploits this by\nbatching block-rows from the same cost type. The block structure also enables efficient\nBlock-Jacobi preconditioning, since diagonal blocks of $J^T J$ can be accumulated\nwithout materializing the full matrix."
  },
  {
   "cell_type": "markdown",
   "id": "kayn4eg6v4",
   "source": "Using the same example from above:\n\n$$\nJ = \\left[\\begin{array}{cc|cc|cc}\n\\color{#1976d2}{0.8} & \\color{#1976d2}{0.3} & 0 & 0 & 0 & 0 \\\\\n\\color{#1976d2}{0.1} & \\color{#1976d2}{0.9} & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & \\color{#1976d2}{0.6} & \\color{#1976d2}{0.2} \\\\\n0 & 0 & 0 & 0 & \\color{#1976d2}{0.4} & \\color{#1976d2}{0.7} \\\\\n\\hline\n\\color{#f57c00}{1.2} & \\color{#f57c00}{0.4} & \\color{#f57c00}{0.7} & \\color{#f57c00}{0.2} & 0 & 0 \\\\\n\\color{#f57c00}{0.5} & \\color{#f57c00}{0.6} & \\color{#f57c00}{0.3} & \\color{#f57c00}{0.8} & 0 & 0 \\\\\n0 & 0 & \\color{#f57c00}{0.9} & \\color{#f57c00}{0.1} & \\color{#f57c00}{0.4} & \\color{#f57c00}{0.7} \\\\\n0 & 0 & \\color{#f57c00}{0.2} & \\color{#f57c00}{0.5} & \\color{#f57c00}{0.8} & \\color{#f57c00}{0.3} \\\\\n\\hline\n\\color{#388e3c}{0.3} & \\color{#388e3c}{0.9} & 0 & 0 & \\color{#388e3c}{0.5} & \\color{#388e3c}{0.1} \\\\\n\\color{#388e3c}{0.7} & \\color{#388e3c}{0.4} & 0 & 0 & \\color{#388e3c}{0.2} & \\color{#388e3c}{0.8}\n\\end{array}\\right]\n\\begin{array}{l}\n\\leftarrow A_0 \\\\\n\\\\\n\\leftarrow A_1 \\\\\n\\\\\n\\leftarrow B_0 \\\\\n\\\\\n\\leftarrow B_1 \\\\\n\\\\\n\\leftarrow C_0 \\\\\n\\end{array}\n$$\n\nEach cost type becomes one `SparseBlockRow`:\n\n```python\n@jdc.pytree_dataclass\nclass BlockRowSparseMatrix:\n    block_rows: tuple[SparseBlockRow, ...]  # One per cost type.\n\n@jdc.pytree_dataclass\nclass SparseBlockRow:\n    num_cols: jdc.Static[int]                      # Total matrix width.\n    block_num_cols: jdc.Static[tuple[int, ...]]    # Block widths.\n    start_cols: tuple[jax.Array, ...]              # Column positions (traced).\n    blocks_concat: jax.Array                       # Jacobian values (traced).\n```\n\n| Cost type | `blocks_concat` shape | `block_num_cols` | `start_cols` | \n|-----------|----------------------|------------------|--------------|\n| A | `(2, 2, 2)` | `(2,)` | `[[0], [4]]` |\n| B | `(2, 2, 4)` | `(2, 2)` | `[[0, 2], [2, 4]]` |\n| C | `(1, 2, 4)` | `(2, 2)` | `[[0, 4]]` |\n\nThe zeros are never stored. `start_cols` records where each block lands in the full matrix.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "x098knls25",
   "source": "### SparseCooMatrix and SparseCsrMatrix\n\njaxls also supports standard sparse formats for testing, benchmarking, and\ninteroperability. CSR is additionally required for the CHOLMOD direct solver.\n\n**SparseCooMatrix** (coordinate format, convertible to JAX's native `BCOO`):\n\n```python\n@jdc.pytree_dataclass\nclass SparseCooMatrix:\n    values: jax.Array              # Shape: (nnz,).\n    coords: SparseCooCoordinates   # Row and column indices.\n```\n\n**SparseCsrMatrix** (compressed sparse row, convertible to JAX's native `BCSR`):\n\n```python\n@jdc.pytree_dataclass\nclass SparseCsrMatrix:\n    values: jax.Array              # Shape: (nnz,).\n    coords: SparseCsrCoordinates   # indices and indptr arrays.\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": "## Linear solvers\n\nThe Gauss-Newton update requires solving $(J^T J + \\lambda I) \\Delta x = -J^T r$.\njaxls offers three linear solvers for this system:\n\n### Conjugate gradient (default)\n\n```python\nsolution = problem.solve(linear_solver=\"conjugate_gradient\")\n```\n\nThis is the default and generally recommended solver.\n\n- **Best for**: Large sparse problems, GPU acceleration\n- **Pros**: Memory-efficient (never forms $J^T J$), JAX-native, works on GPU\n- **Cons**: May require many iterations, sensitive to conditioning\n\nUses inexact Newton via [Eisenstat-Walker](https://doi.org/10.1137/0917003) adaptive tolerances.\n\n### Dense Cholesky\n\n```python\nsolution = problem.solve(linear_solver=\"dense_cholesky\")\n```\n\n- **Best for**: Small problems (<1000 variables)\n- **Pros**: Direct solve, no iterations, numerically stable\n- **Cons**: Forms dense $J^T J$ matrix, $O(n^3)$ complexity\n\n### Sparse Cholesky (CHOLMOD)\n\n```python\nsolution = problem.solve(linear_solver=\"cholmod\")\n```\n\n- **Best for**: Large sparse problems on CPU\n- **Pros**: Direct solve, exploits sparsity, sparse fill-reducing ordering\n- **Cons**: CPU only, requires SuiteSparse installation\n\njaxls caches the symbolic factorization (sparsity pattern analysis), so repeated\nsolves with the same structure are fast."
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": "## Preconditioning\n\nFor conjugate gradient, preconditioning accelerates convergence by transforming\nthe linear system. Instead of solving $Ax = b$ directly, we solve\n$M^{-1}Ax = M^{-1}b$ where the preconditioner $M \\approx A$ is easy to invert.\n\nFor the normal equations $A = J^T J$, jaxls provides two preconditioners:\n\n### Block Jacobi (default)\n\n```python\nsolution = problem.solve(\n    linear_solver=\"conjugate_gradient\",\n    conjugate_gradient=jaxls.ConjugateGradientConfig(\n        preconditioner=\"block_jacobi\",\n    ),\n)\n```\n\nUses $M = \\text{blockdiag}(J^T J)$, where blocks correspond to individual variables.\nFor variable $i$ with Jacobian columns $J_i$:\n\n$$M_i = J_i^T J_i$$\n\nEffective when variables of the same type have similar scales.\n\n### Point Jacobi\n\n```python\nsolution = problem.solve(\n    linear_solver=\"conjugate_gradient\",\n    conjugate_gradient=jaxls.ConjugateGradientConfig(\n        preconditioner=\"point_jacobi\",\n    ),\n)\n```\n\nUses $M = \\text{diag}(J^T J)$, a scalar per tangent dimension:\n\n$$M_{ii} = \\sum_j J_{ji}^2$$\n\nCheaper to compute but less effective than block Jacobi."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}