{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# What's traced?\n\nJAX's JIT compilation requires distinguishing between traced values (that can\nchange between calls) and static values (that trigger recompilation if changed).\n\nThis page discusses:\n- What's traced vs static in jaxls\n- Why variable IDs are traced\n- What triggers recompilation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from loguru import logger\n",
    "\n",
    "logger.remove()\n",
    "logger.add(sys.stdout, format=\"<level>{level: <8}</level> | {message}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "## Background: JAX tracing\n\nWhen JAX JIT-compiles a function, it traces the computation with abstract values.\nValues can be:\n\n- Traced: Represented as abstract shapes/dtypes during compilation. The actual\n  values are substituted at runtime. Changing traced values doesn't trigger recompilation.\n\n- Static: Baked into the compiled code. Changing static values triggers a new\n  compilation (cache miss).\n\nThe challenge is choosing wisely what's static vs traced to balance:\n- Compilation time: Static values enable more optimization but cause more recompiles.\n- Flexibility: Traced values allow runtime changes without recompilation."
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": "## jaxls's static/traced split\n\n### Traced (can change without recompilation)\n\n| Value | Why traced |\n|-------|------------|\n| Variable IDs (`Var.id`) | Different variable subsets use same code |\n| Variable values (`VarValues`) | Values change during optimization |\n| Jacobian values | Computed from current variable values |\n| Solver parameters (LM damping, CG tolerance) | May adapt during solve |\n| Augmented Lagrangian multipliers/penalties | Updated between outer iterations |\n\n### Static (changes trigger recompilation)\n\n| Value | Why static |\n|-------|------------|\n| Problem dimensions (`_tangent_dim`, `_residual_dim`) | Affects array shapes |\n| Cost counts and structure (`_cost_counts`) | Affects loop bounds |\n| Solver choice (`linear_solver`, `sparse_mode`) | Different code paths |\n| Variable tangent dimensions | Affects Jacobian block sizes |\n| Constraint types (equality vs inequality) | Affects AL update logic |\n\nThe pattern: structure is static, values are traced."
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": "## Why variable IDs are traced\n\n`Var.id` is a traced `jax.Array`, not a static `int`. This enables automatic\nvectorization of costs. These two approaches produce equivalent results after analysis:\n\n```python\n# Approach A: Array of IDs (explicitly batched).\ncosts_a = [\n    pairwise_cost(\n        MyVar(id=jnp.arange(100)),      # IDs 0-99\n        MyVar(id=jnp.arange(100) + 1),  # IDs 1-100\n        data,\n    )\n]\n\n# Approach B: List comprehension (implicitly batched).\ncosts_b = [\n    pairwise_cost(MyVar(id=i), MyVar(id=i + 1), data[i])\n    for i in range(100)\n]\n```\n\nIn approach A, the IDs are explicit arrays. In approach B, jaxls stacks the\nscalar IDs into arrays during `analyze()`. Either way, the analyzed problem\nvmaps over the ID arrays to compute residuals and Jacobians in parallel.\n\nThis also works with inline lambda functions:\n\n```python\n# Approach C: Inline lambdas with list comprehension.\ncosts_c = [\n    jaxls.Cost.factory(\n        lambda vals, v: vals[v] - target[i]\n    )(MyVar(id=i))\n    for i in range(100)\n]\n```\n\nEven though this creates 100 separate lambda objects, jaxls recognizes them as\nthe same cost type using bytecode analysis: `dis.Bytecode` extracts the instruction\nsequence, which is identical across all 100 lambdas. Only the closure variables\n(captured `i` values) differ, and these become the traced data that gets stacked.\n\nThis works because `Var.id` is traced: `VarValues` uses `jnp.searchsorted`\nfor O(log n) lookup, which operates on traced arrays without data-dependent\ncontrol flow."
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Implementation: `jdc.Static`\n",
    "\n",
    "jaxls uses `jax_dataclasses` with the `jdc.Static[]` type annotation to mark static fields:\n",
    "\n",
    "```python\n",
    "@jdc.pytree_dataclass\n",
    "class AnalyzedLeastSquaresProblem:\n",
    "    # Traced fields (part of pytree).\n",
    "    _stacked_costs: tuple[_AnalyzedCost, ...]\n",
    "    _sorted_ids_from_var_type: dict[type[Var], jax.Array]\n",
    "    \n",
    "    # Static fields (not part of pytree, trigger recompile if changed).\n",
    "    _cost_counts: jdc.Static[tuple[int, ...]]\n",
    "    _tangent_dim: jdc.Static[int]\n",
    "    _residual_dim: jdc.Static[int]\n",
    "```\n",
    "\n",
    "Static fields are excluded from the pytree structure, so JAX treats them as\n",
    "compile-time constants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": "## What triggers recompilation?\n\n### Does not trigger recompilation\n\n- Different variable IDs (as long as count and types are the same)\n- Different variable values\n- Different initial guesses\n- Different solver parameters (trust region damping, CG tolerance)\n- Different AL penalty parameters\n\n### Triggers recompilation\n\n- Adding/removing variables or costs\n- Changing variable types\n- Changing problem dimensions\n- Switching linear solver (CG â†’ Cholesky)\n- Changing sparse matrix mode\n\nThe first solve is slower (includes compilation), but subsequent solves with\nthe same structure are fast."
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": "## Example: leveraging traced values\n\nBecause variable IDs and solver parameters are traced, the same compiled code\ncan handle different problem configurations.\n\n### Different variable assignments\n\nThe same cost structure can connect different variables without recompilation.\nWith {func}`jax.jit`, the first call compiles and subsequent calls reuse the code:\n\n```python\n@jax.jit\ndef solve_with_ids(\n    var_ids: jax.Array,  # Which variables to use.\n    targets: jax.Array,\n) -> jaxls.VarValues:\n    # Same cost type, but connecting different variable IDs.\n    costs = [prior_cost(MyVar(id=var_ids), targets)]\n    problem = jaxls.LeastSquaresProblem(costs).analyze()\n    return problem.solve(initial_vals).vals\n\n# First call compiles; subsequent calls reuse compiled code.\nsolution1 = solve_with_ids(jnp.array([0, 1, 2]), targets_a)\nsolution2 = solve_with_ids(jnp.array([3, 4, 5]), targets_b)  # No recompilation.\n```\n\n### Batched solves with vmap\n\nWith {func}`jax.vmap`, solve multiple problem instances in parallel by vmapping\nover initial values:\n\n```python\ndef solve_one(init_vals: jaxls.VarValues) -> jaxls.VarValues:\n    return problem.solve(init_vals).vals\n\n# Solve 100 problems in parallel.\nbatched_solutions = jax.jit(jax.vmap(solve_one))(batched_init_vals)\n```\n\n### Sequential solves with scan\n\nWith {func}`jax.lax.scan`, solve a sequence of problems where each uses the\nprevious solution as its initial guess:\n\n```python\ndef solve_step(vals: jaxls.VarValues, target: jax.Array) -> tuple[jaxls.VarValues, jax.Array]:\n    costs = [prior_cost(MyVar(id=jnp.arange(n)), target)]\n    problem = jaxls.LeastSquaresProblem(costs).analyze()\n    new_vals = problem.solve(vals).vals\n    return new_vals, new_vals[MyVar(id=jnp.arange(n))]\n\n# Solve for each target in sequence, warm-starting from previous solution.\nfinal_vals, trajectory = jax.lax.scan(solve_step, initial_vals, targets_sequence)\n```\n\n### Sweeping solver parameters\n\nSolver parameters like damping are traced, so {func}`jax.vmap` can sweep over\nthem without recompilation:\n\n```python\ndef solve_with_damping(damping: float) -> jaxls.VarValues:\n    return problem.solve(\n        initial_vals,\n        trust_region=jaxls.TrustRegionConfig(lambda_initial=damping),\n    ).vals\n\n# Sweep over damping values. All use the same compiled code.\nsolutions = jax.jit(jax.vmap(solve_with_damping))(jnp.logspace(-3, 3, 10))\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}