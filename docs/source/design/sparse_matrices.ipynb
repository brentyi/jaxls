{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Sparse matrices\n",
    "\n",
    "jaxls solves nonlinear least squares using Levenberg-Marquardt, which is based on\n",
    "the Gauss-Newton method.\n",
    "\n",
    "This page discusses:\n",
    "- The Gauss-Newton approximation and why Jacobian structure matters\n",
    "- jaxls's `BlockRowSparseMatrix` representation, which is designed to be GPU-friendly\n",
    "- Linear solvers and preconditioning strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## The Gauss-Newton approximation\n",
    "\n",
    "Newton's method for minimizing $f(x) = \\frac{1}{2}\\sum_i \\|r_i(x)\\|^2$ requires the\n",
    "Hessian $\\nabla^2 f$, which is expensive to compute. Gauss-Newton instead approximates\n",
    "the Hessian using only first-order information:\n",
    "\n",
    "$$\\nabla^2 f \\approx J^\\top J$$\n",
    "\n",
    "where $J = \\partial r / \\partial x$ is the Jacobian of the stacked residual vector\n",
    "$r(x) = [r_0(x)^\\top, r_1(x)^\\top, \\ldots]^\\top$ with respect to the optimization\n",
    "variables.\n",
    "\n",
    "## Why Jacobians are sparse\n",
    "\n",
    "Residual terms in nonlinear least squares often depend on only a subset of the\n",
    "optimization variables. For example, a pairwise cost between two poses only depends\n",
    "on those two poses, not the hundreds of other poses in a SLAM problem. This produces\n",
    "sparse Jacobians: row block $i$ of $J$ has non-zeros only in columns corresponding\n",
    "to the variables that $r_i$ depends on.\n",
    "\n",
    "This sparsity pattern corresponds to the adjacency matrix of a bipartite factor graph:\n",
    "\n",
    "- **Variables** (circles): The parameters we're optimizing\n",
    "- **Costs** (squares): Residual functions that connect to their dependent variables\n",
    "\n",
    "Consider an optimization problem with variables $x_0$, $x_1$, $x_2$ and three types\n",
    "of costs: A (unary), B (binary between adjacent variables), and C (binary between\n",
    "non-adjacent variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Internal implementation.\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "\n",
    "# Create a simple problem structure visualization.\n",
    "# Variables: x0 (dim 2), x1 (dim 2), x2 (dim 2)\n",
    "# Cost types: A (unary), B (binary adjacent), C (binary non-adjacent)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\"Problem Structure\", \"Jacobian Sparsity Pattern\"),\n",
    "    column_widths=[0.45, 0.55],\n",
    "    horizontal_spacing=0.12,\n",
    ")\n",
    "\n",
    "# Problem structure layout.\n",
    "var_x = [0, 2, 4]\n",
    "var_y = [1, 1, 1]\n",
    "var_labels = [\"x₀\", \"x₁\", \"x₂\"]\n",
    "var_dims = [2, 2, 2]  # Tangent dimensions (all 2D).\n",
    "\n",
    "# C₀ placed above the variables for cleaner layout.\n",
    "cost_x = [0, 1, 3, 4, 2]\n",
    "cost_y = [0, 0, 0, 0, 2]\n",
    "cost_labels = [\"A₀\", \"B₀\", \"B₁\", \"A₁\", \"C₀\"]\n",
    "cost_dims = [2, 2, 2, 2, 2]  # Residual dimensions (all 2).\n",
    "# Colors: A costs (blue), B costs (orange), C costs (green).\n",
    "cost_colors = [\"#1976d2\", \"#f57c00\", \"#f57c00\", \"#1976d2\", \"#388e3c\"]\n",
    "\n",
    "# Connections: (cost_idx, var_idx).\n",
    "# A₀→x₀, B₀→x₀,x₁, B₁→x₁,x₂, A₁→x₂, C₀→x₀,x₂\n",
    "edges = [(0, 0), (1, 0), (1, 1), (2, 1), (2, 2), (3, 2), (4, 0), (4, 2)]\n",
    "\n",
    "# Draw edges.\n",
    "for ci, vi in edges:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[cost_x[ci], var_x[vi]],\n",
    "            y=[cost_y[ci], var_y[vi]],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"#888\", width=2),\n",
    "            showlegend=False,\n",
    "            hoverinfo=\"skip\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "# Draw variables (circles).\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=var_x,\n",
    "        y=var_y,\n",
    "        mode=\"markers+text\",\n",
    "        marker=dict(size=40, color=\"#2196F3\", line=dict(width=2, color=\"white\")),\n",
    "        text=var_labels,\n",
    "        textposition=\"middle center\",\n",
    "        textfont=dict(color=\"white\", size=14),\n",
    "        name=\"Variables\",\n",
    "        hovertemplate=\"%{text}<br>dim=%{customdata}<extra></extra>\",\n",
    "        customdata=var_dims,\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "# Draw costs (squares) - color by cost type.\n",
    "for i, (cx, cy, label, color, dim) in enumerate(\n",
    "    zip(cost_x, cost_y, cost_labels, cost_colors, cost_dims)\n",
    "):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[cx],\n",
    "            y=[cy],\n",
    "            mode=\"markers+text\",\n",
    "            marker=dict(\n",
    "                size=35, color=color, symbol=\"square\", line=dict(width=2, color=\"white\")\n",
    "            ),\n",
    "            text=[label],\n",
    "            textposition=\"middle center\",\n",
    "            textfont=dict(color=\"white\", size=12),\n",
    "            name=label,\n",
    "            hovertemplate=f\"{label}<br>residual dim={dim}<extra></extra>\",\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "# Build Jacobian sparsity pattern.\n",
    "# Order costs by type for the matrix: A₀, A₁, B₀, B₁, C₀\n",
    "cost_order = [0, 3, 1, 2, 4]  # Reorder to group by type.\n",
    "ordered_cost_dims = [cost_dims[i] for i in cost_order]\n",
    "ordered_cost_labels = [cost_labels[i] for i in cost_order]\n",
    "ordered_cost_colors_idx = [0, 0, 1, 1, 2]  # A, A, B, B, C\n",
    "\n",
    "n_rows = sum(ordered_cost_dims)  # 10\n",
    "n_cols = sum(var_dims)  # 6\n",
    "\n",
    "row_starts = [0] + list(np.cumsum(ordered_cost_dims)[:-1])\n",
    "col_starts = [0] + list(np.cumsum(var_dims)[:-1])\n",
    "\n",
    "# Build edge map for reordered costs.\n",
    "# Original edges: A₀→x₀, B₀→x₀,x₁, B₁→x₁,x₂, A₁→x₂, C₀→x₀,x₂\n",
    "# Reordered: A₀→x₀, A₁→x₂, B₀→x₀,x₁, B₁→x₁,x₂, C₀→x₀,x₂\n",
    "reordered_edges = [\n",
    "    (0, [0]),  # A₀ → x₀\n",
    "    (1, [2]),  # A₁ → x₂\n",
    "    (2, [0, 1]),  # B₀ → x₀, x₁\n",
    "    (3, [1, 2]),  # B₁ → x₁, x₂\n",
    "    (4, [0, 2]),  # C₀ → x₀, x₂ (non-adjacent!)\n",
    "]\n",
    "\n",
    "# Create color matrix for heatmap.\n",
    "color_matrix = np.zeros((n_rows, n_cols))\n",
    "color_vals = [1, 1, 2, 2, 3]  # A=1 (blue), B=2 (orange), C=3 (green)\n",
    "for row_idx, (_, var_indices) in enumerate(reordered_edges):\n",
    "    r0, r1 = row_starts[row_idx], row_starts[row_idx] + ordered_cost_dims[row_idx]\n",
    "    for vi in var_indices:\n",
    "        c0, c1 = col_starts[vi], col_starts[vi] + var_dims[vi]\n",
    "        color_matrix[r0:r1, c0:c1] = color_vals[row_idx]\n",
    "\n",
    "# Custom colorscale: white, blue, orange, green.\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=color_matrix[::-1],\n",
    "        colorscale=[[0, \"white\"], [0.33, \"#1976d2\"], [0.67, \"#f57c00\"], [1, \"#388e3c\"]],\n",
    "        showscale=False,\n",
    "        zmin=0,\n",
    "        zmax=3,\n",
    "        hovertemplate=\"row %{y}, col %{x}<extra></extra>\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "# Add grid lines for Jacobian.\n",
    "for i in range(n_rows + 1):\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=-0.5,\n",
    "        x1=n_cols - 0.5,\n",
    "        y0=i - 0.5,\n",
    "        y1=i - 0.5,\n",
    "        line=dict(color=\"#ddd\", width=1),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "for j in range(n_cols + 1):\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=j - 0.5,\n",
    "        x1=j - 0.5,\n",
    "        y0=-0.5,\n",
    "        y1=n_rows - 0.5,\n",
    "        line=dict(color=\"#ddd\", width=1),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "# Add block separators (between cost types).\n",
    "type_boundaries = [4, 8]  # After A costs, after B costs\n",
    "for rs in type_boundaries:\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=-0.5,\n",
    "        x1=n_cols - 0.5,\n",
    "        y0=n_rows - rs - 0.5,\n",
    "        y1=n_rows - rs - 0.5,\n",
    "        line=dict(color=\"#999\", width=2),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "for cs in col_starts[1:]:\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=cs - 0.5,\n",
    "        x1=cs - 0.5,\n",
    "        y0=-0.5,\n",
    "        y1=n_rows - 0.5,\n",
    "        line=dict(color=\"#2196F3\", width=2),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "# Add column labels (variables) at bottom.\n",
    "col_centers = [col_starts[i] + var_dims[i] / 2 - 0.5 for i in range(len(var_dims))]\n",
    "for i, (cx, label) in enumerate(zip(col_centers, var_labels)):\n",
    "    fig.add_annotation(\n",
    "        x=cx,\n",
    "        y=-1.0,\n",
    "        text=label,\n",
    "        showarrow=False,\n",
    "        font=dict(size=14),\n",
    "        xref=\"x2\",\n",
    "        yref=\"y2\",\n",
    "    )\n",
    "\n",
    "# Add row labels (costs) at left - show type groups.\n",
    "type_labels = [\"A\", \"B\", \"C\"]\n",
    "type_row_ranges = [(0, 4), (4, 8), (8, 10)]\n",
    "for label, (r0, r1) in zip(type_labels, type_row_ranges):\n",
    "    ry = n_rows - (r0 + r1) / 2 - 0.5\n",
    "    fig.add_annotation(\n",
    "        x=-0.8,\n",
    "        y=ry,\n",
    "        text=label,\n",
    "        showarrow=False,\n",
    "        font=dict(size=12),\n",
    "        xref=\"x2\",\n",
    "        yref=\"y2\",\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(showgrid=False, zeroline=False, showticklabels=False, row=1, col=1)\n",
    "fig.update_yaxes(showgrid=False, zeroline=False, showticklabels=False, row=1, col=1)\n",
    "fig.update_xaxes(showgrid=False, zeroline=False, showticklabels=False, row=1, col=2)\n",
    "fig.update_yaxes(showgrid=False, zeroline=False, showticklabels=False, row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=350,\n",
    "    margin=dict(t=40, b=60, l=60, r=40),\n",
    "    showlegend=False,\n",
    ")\n",
    "\n",
    "HTML(fig.to_html(full_html=False, include_plotlyjs=\"cdn\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "In this example, the Jacobian is ~47% zeros, and this ratio grows with problem size.\n",
    "For large SLAM or bundle adjustment problems, Jacobians can be >99% sparse. Exploiting\n",
    "this sparsity is critical for scaling to large problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Sparse matrix formats\n",
    "\n",
    "jaxls implements three sparse matrix representations. You can select between them\n",
    "using the `sparse_mode` parameter:\n",
    "\n",
    "```python\n",
    "# Default: block-row format (best for CG on GPU).\n",
    "solution = problem.solve(sparse_mode=\"blockrow\")\n",
    "\n",
    "# COO format (converts to JAX BCOO).\n",
    "solution = problem.solve(sparse_mode=\"coo\")\n",
    "\n",
    "# CSR format (always used for CHOLMOD).\n",
    "solution = problem.solve(sparse_mode=\"csr\")\n",
    "```\n",
    "\n",
    "### BlockRowSparseMatrix\n",
    "\n",
    "jaxls's default and recommended sparse matrix representation uses what we call\n",
    "sparse \"block-rows\". Costs of the same type have Jacobians with identical structure:\n",
    "same block sizes, just different values and column positions. jaxls exploits this by\n",
    "batching block-rows from the same cost type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kayn4eg6v4",
   "metadata": {},
   "source": [
    "Using the same example from above:\n",
    "\n",
    "$$\n",
    "J = \\left[\\begin{array}{cc|cc|cc}\n",
    "\\color{#1976d2}{0.8} & \\color{#1976d2}{0.3} & 0 & 0 & 0 & 0 \\\\\n",
    "\\color{#1976d2}{0.1} & \\color{#1976d2}{0.9} & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & \\color{#1976d2}{0.6} & \\color{#1976d2}{0.2} \\\\\n",
    "0 & 0 & 0 & 0 & \\color{#1976d2}{0.4} & \\color{#1976d2}{0.7} \\\\\n",
    "\\hline\n",
    "\\color{#f57c00}{1.2} & \\color{#f57c00}{0.4} & \\color{#f57c00}{0.7} & \\color{#f57c00}{0.2} & 0 & 0 \\\\\n",
    "\\color{#f57c00}{0.5} & \\color{#f57c00}{0.6} & \\color{#f57c00}{0.3} & \\color{#f57c00}{0.8} & 0 & 0 \\\\\n",
    "0 & 0 & \\color{#f57c00}{0.9} & \\color{#f57c00}{0.1} & \\color{#f57c00}{0.4} & \\color{#f57c00}{0.7} \\\\\n",
    "0 & 0 & \\color{#f57c00}{0.2} & \\color{#f57c00}{0.5} & \\color{#f57c00}{0.8} & \\color{#f57c00}{0.3} \\\\\n",
    "\\hline\n",
    "\\color{#388e3c}{0.3} & \\color{#388e3c}{0.9} & 0 & 0 & \\color{#388e3c}{0.5} & \\color{#388e3c}{0.1} \\\\\n",
    "\\color{#388e3c}{0.7} & \\color{#388e3c}{0.4} & 0 & 0 & \\color{#388e3c}{0.2} & \\color{#388e3c}{0.8}\n",
    "\\end{array}\\right]\n",
    "\\begin{array}{l}\n",
    "\\leftarrow A_0 \\\\\n",
    "\\\\\n",
    "\\leftarrow A_1 \\\\\n",
    "\\\\\n",
    "\\leftarrow B_0 \\\\\n",
    "\\\\\n",
    "\\leftarrow B_1 \\\\\n",
    "\\\\\n",
    "\\leftarrow C_0 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Each cost type becomes one `SparseBlockRow`:\n",
    "\n",
    "```python\n",
    "@jdc.pytree_dataclass\n",
    "class BlockRowSparseMatrix:\n",
    "    block_rows: tuple[SparseBlockRow, ...]  # One per cost type.\n",
    "\n",
    "@jdc.pytree_dataclass\n",
    "class SparseBlockRow:\n",
    "    num_cols: jdc.Static[int]                      # Total matrix width.\n",
    "    block_num_cols: jdc.Static[tuple[int, ...]]    # Block widths.\n",
    "    start_cols: tuple[jax.Array, ...]              # Column positions (traced).\n",
    "    blocks_concat: jax.Array                       # Jacobian values (traced).\n",
    "```\n",
    "\n",
    "| Cost type | `blocks_concat` shape | `block_num_cols` | `start_cols` | \n",
    "|-----------|----------------------|------------------|--------------|\n",
    "| A | `(2, 2, 2)` | `(2,)` | `[[0], [4]]` |\n",
    "| B | `(2, 2, 4)` | `(2, 2)` | `[[0, 2], [2, 4]]` |\n",
    "| C | `(1, 2, 4)` | `(2, 2)` | `[[0, 4]]` |\n",
    "\n",
    "The zeros are never stored. `start_cols` records where each block lands in the full matrix.\n",
    "\n",
    "The block-row format is more GPU-friendly than traditional sparse formats like COO\n",
    "and CSR, which index individual nonzero entries. This leads to irregular\n",
    "memory access patterns and poor cache utilization on GPUs, where performance depends\n",
    "on coalesced memory access and high arithmetic intensity. The block-row format\n",
    "sidesteps these issues:\n",
    "\n",
    "- **Dense matmul kernels.** Jacobian blocks from each cost type are stacked into\n",
    "  dense arrays (`blocks_concat`). Matrix-vector products become batched dense\n",
    "  matmuls, which map directly to optimized GPU kernels (cuBLAS, etc.).\n",
    "- **Structured indexing.** While `start_cols` involves dynamic indexing for\n",
    "  gather/scatter operations, the block structure (number of blocks, their sizes)\n",
    "  is static. This is more regular than arbitrary element-wise sparse indexing.\n",
    "- **Efficient preconditioning.** Diagonal blocks of $J^T J$ can be accumulated\n",
    "  without materializing the full matrix, enabling Block-Jacobi preconditioning\n",
    "  with minimal overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x098knls25",
   "metadata": {},
   "source": [
    "### SparseCooMatrix and SparseCsrMatrix\n",
    "\n",
    "jaxls also supports standard sparse formats for compatibility with JAX's native\n",
    "sparse operations and external solvers like CHOLMOD.\n",
    "\n",
    "**SparseCooMatrix** (coordinate format):\n",
    "\n",
    "Stores each nonzero as a `(row, col, value)` triple. Simple but inefficient for\n",
    "matrix operations since entries aren't ordered.\n",
    "\n",
    "```python\n",
    "@jdc.pytree_dataclass\n",
    "class SparseCooCoordinates:\n",
    "    rows: jax.Array   # Row indices, shape (nnz,).\n",
    "    cols: jax.Array   # Column indices, shape (nnz,).\n",
    "    shape: tuple[int, int]\n",
    "\n",
    "@jdc.pytree_dataclass\n",
    "class SparseCooMatrix:\n",
    "    values: jax.Array              # Nonzero values, shape (nnz,).\n",
    "    coords: SparseCooCoordinates   # Row and column indices.\n",
    "```\n",
    "\n",
    "Use `sparse_mode=\"coo\"` to convert to JAX's native `BCOO` format for sparse\n",
    "linear algebra operations.\n",
    "\n",
    "**SparseCsrMatrix** (compressed sparse row):\n",
    "\n",
    "Groups entries by row for efficient row-wise access. Each row's column indices\n",
    "and values are stored contiguously, with `indptr` marking where each row starts.\n",
    "\n",
    "```python\n",
    "@jdc.pytree_dataclass  \n",
    "class SparseCsrCoordinates:\n",
    "    indices: jax.Array   # Column indices, shape (nnz,).\n",
    "    indptr: jax.Array    # Row pointers, shape (nrows + 1,).\n",
    "    shape: tuple[int, int]\n",
    "\n",
    "@jdc.pytree_dataclass\n",
    "class SparseCsrMatrix:\n",
    "    values: jax.Array              # Nonzero values, shape (nnz,).\n",
    "    coords: SparseCsrCoordinates   # CSR index structure.\n",
    "```\n",
    "\n",
    "CSR is required for CHOLMOD (`linear_solver=\"cholmod\"`), which performs sparse\n",
    "Cholesky factorization on CPU. Conversion to CSR is done automatically when using\n",
    "CHOLMOD, but you can also explicitly request it with `sparse_mode=\"csr\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Linear solvers\n",
    "\n",
    "The Gauss-Newton update requires solving $(J^T J + \\lambda I) \\Delta x = -J^T r$.\n",
    "jaxls offers three linear solvers for this system:\n",
    "\n",
    "### Conjugate gradient (default)\n",
    "\n",
    "```python\n",
    "solution = problem.solve(linear_solver=\"conjugate_gradient\")\n",
    "```\n",
    "\n",
    "This is the default and generally recommended solver.\n",
    "\n",
    "- **Best for**: Large sparse problems, GPU acceleration\n",
    "- **Pros**: Memory-efficient (never forms $J^T J$), JAX-native, works on GPU\n",
    "- **Cons**: May require many iterations, sensitive to conditioning\n",
    "\n",
    "Uses inexact Newton via [Eisenstat-Walker](https://doi.org/10.1137/0917003) adaptive tolerances.\n",
    "\n",
    "### Dense Cholesky\n",
    "\n",
    "```python\n",
    "solution = problem.solve(linear_solver=\"dense_cholesky\")\n",
    "```\n",
    "\n",
    "- **Best for**: Small problems (<1000 variables)\n",
    "- **Pros**: Direct solve, no iterations, numerically stable\n",
    "- **Cons**: Forms dense $J^T J$ matrix, $O(n^3)$ complexity\n",
    "\n",
    "### Sparse Cholesky (CHOLMOD)\n",
    "\n",
    "```python\n",
    "solution = problem.solve(linear_solver=\"cholmod\")\n",
    "```\n",
    "\n",
    "- **Best for**: Large sparse problems on CPU\n",
    "- **Pros**: Direct solve, exploits sparsity, sparse fill-reducing ordering\n",
    "- **Cons**: CPU only, requires SuiteSparse installation\n",
    "\n",
    "jaxls caches the symbolic factorization (sparsity pattern analysis), so repeated\n",
    "solves with the same structure are fast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Preconditioning\n",
    "\n",
    "For conjugate gradient, preconditioning accelerates convergence by transforming\n",
    "the linear system. Instead of solving $Ax = b$ directly, we solve\n",
    "$M^{-1}Ax = M^{-1}b$ where the preconditioner $M \\approx A$ is easy to invert.\n",
    "\n",
    "For the normal equations $A = J^T J$, jaxls provides two preconditioners:\n",
    "\n",
    "### Block Jacobi (default)\n",
    "\n",
    "```python\n",
    "solution = problem.solve(\n",
    "    linear_solver=jaxls.ConjugateGradientConfig(\n",
    "        preconditioner=\"block_jacobi\",\n",
    "    ),\n",
    ")\n",
    "```\n",
    "\n",
    "Uses $M = \\text{blockdiag}(J^T J)$, where blocks correspond to individual variables.\n",
    "For variable $i$ with Jacobian columns $J_i$:\n",
    "\n",
    "$$M_i = J_i^T J_i$$\n",
    "\n",
    "Effective when off-diagonal coupling between variables is weak.\n",
    "\n",
    "### Point Jacobi\n",
    "\n",
    "```python\n",
    "solution = problem.solve(\n",
    "    linear_solver=jaxls.ConjugateGradientConfig(\n",
    "        preconditioner=\"point_jacobi\",\n",
    "    ),\n",
    ")\n",
    "```\n",
    "\n",
    "Uses $M = \\text{diag}(J^T J)$, a scalar per tangent dimension:\n",
    "\n",
    "$$M_{ii} = \\sum_j J_{ji}^2$$\n",
    "\n",
    "Cheaper to compute but less effective than block Jacobi.\n",
    "\n",
    "## Related pages\n",
    "\n",
    "- {doc}`../guide/tips_and_gotchas`: Practical guidance on solver and Jacobian mode selection\n",
    "- {doc}`../guide/advanced/custom_jacobians`: Providing analytical Jacobians\n",
    "- {doc}`traced_vs_static`: What values are traced vs static in JAX compilation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
