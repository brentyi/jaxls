{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d59bc02",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "source": "# What's traced?\n\nJAX's JIT compilation requires distinguishing between traced values (that can\nchange between calls) and static values (that trigger recompilation if changed).\n\nThis page discusses:\n- What's traced vs static in jaxls\n- Why variable IDs are traced\n- What triggers recompilation\n- Examples: leveraging traced values"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Background: JAX tracing\n",
    "\n",
    "When JAX JIT-compiles a function, it traces the computation with abstract values.\n",
    "Values can be:\n",
    "\n",
    "- Traced: Represented as abstract shapes/dtypes during compilation. The actual\n",
    "  values are substituted at runtime. Changing traced values doesn't trigger recompilation.\n",
    "\n",
    "- Static: Baked into the compiled code. Changing static values triggers a new\n",
    "  compilation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## jaxls's static/traced split\n",
    "\n",
    "### Traced (can change without recompilation)\n",
    "\n",
    "| Value | Why traced |\n",
    "|-------|------------|\n",
    "| Variable IDs (`Var.id`) | Different variable subsets use same code |\n",
    "| Variable values (`VarValues`) | Values change during optimization |\n",
    "| Jacobian values | Computed from current variable values |\n",
    "| Solver parameters (LM damping, CG tolerance) | May adapt during solve |\n",
    "| Augmented Lagrangian multipliers/penalties | Updated between outer iterations |\n",
    "\n",
    "### Static (changes trigger recompilation)\n",
    "\n",
    "| Value | Why static |\n",
    "|-------|------------|\n",
    "| Problem dimensions (`_tangent_dim`, `_residual_dim`) | Determines array shapes |\n",
    "| Cost counts and structure (`_cost_counts`) | Determines vectorization structure |\n",
    "| Solver choice (`linear_solver`, `sparse_mode`) | Different code paths |\n",
    "| Variable tangent dimensions | Determines Jacobian block sizes |\n",
    "| Constraint types (equality vs inequality) | Different augmented Lagrangian update logic |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": "## Why variable IDs are traced\n\n`Var.id` is a traced `jax.Array`, not a static `int`. This enables automatic\nvectorization of costs. These two approaches produce equivalent results after analysis:\n\n```python\n# Approach A: Array of IDs (explicitly batched).\ncosts_a = [\n    pairwise_cost(\n        MyVar(id=jnp.arange(100)),      # IDs 0-99\n        MyVar(id=jnp.arange(100) + 1),  # IDs 1-100\n        data,\n    )\n]\n\n# Approach B: List comprehension (implicitly batched).\ncosts_b = [\n    pairwise_cost(MyVar(id=i), MyVar(id=i + 1), data[i])\n    for i in range(100)\n]\n```\n\nIn approach A, the IDs are explicit arrays. In approach B, jaxls stacks the\nscalar IDs into arrays during `analyze()`. In both cases, the analyzed problem\nvmaps over the ID arrays to compute residuals and Jacobians in parallel.\n\nThis also works with inline lambda functions:\n\n```python\n# Approach C: Inline lambdas with list comprehension.\ncosts_c = [\n    jaxls.Cost.factory(\n        lambda vals, v: vals[v] - target[i]\n    )(MyVar(id=i))\n    for i in range(100)\n]\n```\n\nEven though this creates 100 separate lambda objects, jaxls recognizes them as\nthe same cost type using bytecode analysis: `dis.Bytecode` extracts the instruction\nsequence, which is identical across all 100 lambdas."
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Examples: leveraging traced values\n",
    "\n",
    "### Different variable assignments\n",
    "\n",
    "The same cost structure can connect different variables without recompilation.\n",
    "{func}`jax.jit` will only compile the first call:\n",
    "\n",
    "```python\n",
    "@jax.jit\n",
    "def solve_with_ids(\n",
    "    var_ids: jax.Array,  # Which variables to use.\n",
    "    targets: jax.Array,\n",
    ") -> jaxls.VarValues:\n",
    "    # Same cost type, but connecting different variable IDs.\n",
    "    costs = [prior_cost(MyVar(id=var_ids), targets)]\n",
    "    problem = jaxls.LeastSquaresProblem(costs).analyze()\n",
    "    return problem.solve(initial_vals).vals\n",
    "\n",
    "# First call compiles; subsequent calls reuse compiled code.\n",
    "solution1 = solve_with_ids(jnp.array([0, 1, 2]), targets_a)\n",
    "solution2 = solve_with_ids(jnp.array([3, 4, 5]), targets_b)  # No recompilation.\n",
    "```\n",
    "\n",
    "### Batched solves with vmap\n",
    "\n",
    "With {func}`jax.vmap`, solve multiple problem instances in parallel by vmapping\n",
    "over initial values:\n",
    "\n",
    "```python\n",
    "def solve_one(init_vals: jaxls.VarValues) -> jaxls.VarValues:\n",
    "    return problem.solve(init_vals).vals\n",
    "\n",
    "# Solve 100 problems in parallel.\n",
    "batched_solutions = jax.jit(jax.vmap(solve_one))(batched_init_vals)\n",
    "```\n",
    "\n",
    "### Sequential solves with scan\n",
    "\n",
    "With {func}`jax.lax.scan`, solve a sequence of problems where each uses the\n",
    "previous solution as its initial guess:\n",
    "\n",
    "```python\n",
    "def solve_step(vals: jaxls.VarValues, target: jax.Array) -> tuple[jaxls.VarValues, jax.Array]:\n",
    "    costs = [prior_cost(MyVar(id=jnp.arange(n)), target)]\n",
    "    problem = jaxls.LeastSquaresProblem(costs).analyze()\n",
    "    new_vals = problem.solve(vals).vals\n",
    "    return new_vals, new_vals[MyVar(id=jnp.arange(n))]\n",
    "\n",
    "# Solve for each target in sequence, warm-starting from previous solution.\n",
    "final_vals, trajectory = jax.lax.scan(solve_step, initial_vals, targets_sequence)\n",
    "```\n",
    "\n",
    "### Sweeping solver parameters\n",
    "\n",
    "Traced values can be sweeped over in parallel with {func}`jax.vmap`:\n",
    "\n",
    "```python\n",
    "def solve_with_damping(damping: float) -> jaxls.VarValues:\n",
    "    return problem.solve(\n",
    "        initial_vals,\n",
    "        trust_region=jaxls.TrustRegionConfig(lambda_initial=damping),\n",
    "    ).vals\n",
    "\n",
    "# Sweep over damping values.\n",
    "solutions = jax.jit(jax.vmap(solve_with_damping))(jnp.logspace(-3, 3, 10))\n",
    "```\n",
    "\n",
    "## Related pages\n",
    "\n",
    "- {doc}`../guide/tips_and_gotchas`: Batched construction and performance tips\n",
    "- {doc}`sparse_matrices`: How Jacobian sparsity is exploited"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}